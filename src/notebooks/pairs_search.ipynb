{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e90f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import coint, adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66513769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "DATA_DIR = 'data_raw' # Your folder name\n",
    "TIMEFRAME = '15m'      # The timeframe you want to test\n",
    "TOKENS = ['btc', 'sol', 'sui', 'avax', 'trx', 'uni', 'doge', 'xrp'] # I assumed 'unidoge' was UNI and DOGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56dd22c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 1. DATA LOADING ---\n",
    "def load_crypto_data(tokens, timeframe):\n",
    "    \"\"\"\n",
    "    Reads CSVs and aligns them by timestamp to create a single 'Close Price' matrix.\n",
    "    \"\"\"\n",
    "    df_combined = pd.DataFrame()\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Construct path based on your structure: data_raw/btc/BTCUSDT_1d.csv\n",
    "        # Adjust path construction if your folder structure is slightly different\n",
    "        file_path = f\"../../{DATA_DIR}/{token.lower()}/{token.upper()}USDT_{timeframe}.csv\"\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"‚ö†Ô∏è Warning: File not found for {token} at {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Read Data\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert timestamp (Binance uses ms timestamps usually)\n",
    "        # If your 'open_time' is like '2024-12-29', pandas handles it automatically mostly\n",
    "        df['datetime'] = pd.to_datetime(df['open_time'])\n",
    "        df.set_index('datetime', inplace=True)\n",
    "        \n",
    "        # We only care about Close price for this analysis\n",
    "        df_combined[token] = df['close']\n",
    "    \n",
    "    # Drop rows with NaN (alignment issues) to ensure fair comparison\n",
    "    df_combined.dropna(inplace=True)\n",
    "    print(f\"‚úÖ Loaded {len(tokens)} tokens. Shared data points: {len(df_combined)} rows.\")\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1855feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. STATISTICAL TESTS ---\n",
    "\n",
    "def calculate_hurst(series):\n",
    "    \"\"\"\n",
    "    Calculates the Hurst Exponent to check for Mean Reversion.\n",
    "    H < 0.5 = Mean Reverting\n",
    "    H = 0.5 = Random Walk\n",
    "    H > 0.5 = Trending\n",
    "    \"\"\"\n",
    "    lags = range(2, 100)\n",
    "    tau = [np.sqrt(np.std(np.subtract(series[lag:], series[:-lag]))) for lag in lags]\n",
    "    poly = np.polyfit(np.log(lags), np.log(tau), 1)\n",
    "    return poly[0] * 2.0\n",
    "\n",
    "def calculate_half_life(spread):\n",
    "    \"\"\"\n",
    "    Calculates how long it takes for the spread to revert halfway to the mean.\n",
    "    \"\"\"\n",
    "    spread_lag = spread.shift(1)\n",
    "    spread_lag.iloc[0] = spread_lag.iloc[1]\n",
    "    spread_ret = spread - spread_lag\n",
    "    spread_ret.iloc[0] = spread_ret.iloc[1]\n",
    "    \n",
    "    model = sm.OLS(spread_ret, sm.add_constant(spread_lag))\n",
    "    res = model.fit()\n",
    "    # Access by position using .iloc[1] instead of [1] since params is a Series with named indices\n",
    "    lambda_param = res.params.iloc[1]\n",
    "    \n",
    "    if lambda_param >= 0: return np.inf # Non-reverting\n",
    "    return -np.log(2) / lambda_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a758437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cointegrated_pairs(data):\n",
    "    \"\"\"\n",
    "    Tests every possible pair for cointegration.\n",
    "    \"\"\"\n",
    "    n = data.shape[1]\n",
    "    keys = data.keys()\n",
    "    results = []\n",
    "    \n",
    "    print(\"üîç Scanning for pairs...\")\n",
    "    \n",
    "    # Iterate through every unique combination (e.g., BTC-SOL, BTC-ETH...)\n",
    "    for asset_a, asset_b in combinations(keys, 2):\n",
    "        series_a = data[asset_a]\n",
    "        series_b = data[asset_b]\n",
    "        \n",
    "        # 1. Engle-Granger Test (Cointegration)\n",
    "        # Null hypothesis: No cointegration. Low p-value means we reject null -> Cointegration exists.\n",
    "        score, p_value, _ = coint(series_a, series_b)\n",
    "        \n",
    "        # 2. Calculate Spread (Hedge Ratio via OLS)\n",
    "        # Spread = AssetA - (HedgeRatio * AssetB)\n",
    "        series_b_const = sm.add_constant(series_b)\n",
    "        result = sm.OLS(series_a, series_b_const).fit()\n",
    "        hedge_ratio = result.params[asset_b]\n",
    "        spread = series_a - (hedge_ratio * series_b)\n",
    "        \n",
    "        # 3. Hurst Exponent of the Spread\n",
    "        hurst = calculate_hurst(spread.values)\n",
    "        \n",
    "        # 4. Half Life\n",
    "        half_life = calculate_half_life(spread)\n",
    "        \n",
    "        results.append({\n",
    "            'Pair': f\"{asset_a}-{asset_b}\",\n",
    "            'P-Value': round(p_value, 5),\n",
    "            'Hurst': round(hurst, 3),\n",
    "            'Half_Life': round(half_life, 2),\n",
    "            'Hedge_Ratio': round(hedge_ratio, 4)\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame and Filter\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # FILTERING LOGIC (The \"Secret Sauce\")\n",
    "    # P-Value < 0.05 (Statistically Significant)\n",
    "    # Hurst < 0.5 (Mean Reverting)\n",
    "    # Half Life > 1 and < 20 (Tradeable frequency)\n",
    "    valid_pairs = results_df[\n",
    "        (results_df['P-Value'] < 0.05) & \n",
    "        (results_df['Hurst'] < 0.5) &\n",
    "        (results_df['Half_Life'] > 1)\n",
    "    ].sort_values(by='P-Value')\n",
    "    \n",
    "    return results_df, valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da98c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 8 tokens. Shared data points: 35040 rows.\n",
      "üîç Scanning for pairs...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m df_prices = load_crypto_data(TOKENS, TIMEFRAME)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 2. Run Analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m all_pairs, best_pairs = \u001b[43mfind_cointegrated_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_prices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 3. Display Results\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müèÜ --- TOP CANDIDATE PAIRS ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mfind_cointegrated_pairs\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m     14\u001b[39m series_b = data[asset_b]\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# 1. Engle-Granger Test (Cointegration)\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Null hypothesis: No cointegration. Low p-value means we reject null -> Cointegration exists.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m score, p_value, _ = \u001b[43mcoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseries_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseries_b\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 2. Calculate Spread (Hedge Ratio via OLS)\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Spread = AssetA - (HedgeRatio * AssetB)\u001b[39;00m\n\u001b[32m     22\u001b[39m series_b_const = sm.add_constant(series_b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/stonks.ai/.venv/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:1817\u001b[39m, in \u001b[36mcoint\u001b[39m\u001b[34m(y0, y1, trend, method, maxlag, autolag, return_results)\u001b[39m\n\u001b[32m   1814\u001b[39m res_co = OLS(y0, xx).fit()\n\u001b[32m   1816\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res_co.rsquared < \u001b[32m1\u001b[39m - \u001b[32m100\u001b[39m * SQRTEPS:\n\u001b[32m-> \u001b[39m\u001b[32m1817\u001b[39m     res_adf = \u001b[43madfuller\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m        \u001b[49m\u001b[43mres_co\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautolag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautolag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   1819\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1821\u001b[39m     warnings.warn(\n\u001b[32m   1822\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33my0 and y1 are (almost) perfectly colinear.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1823\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCointegration test is not reliable in this case.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1824\u001b[39m         CollinearityWarning,\n\u001b[32m   1825\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1826\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/stonks.ai/.venv/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:326\u001b[39m, in \u001b[36madfuller\u001b[39m\u001b[34m(x, maxlag, regression, autolag, store, regresults)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# 1 for level\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# search for lag length with smallest information criteria\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# Note: use the same number of observations to have comparable IC\u001b[39;00m\n\u001b[32m    323\u001b[39m \u001b[38;5;66;03m# aic and bic: smaller is better\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m regresults:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     icbest, bestlag = \u001b[43m_autolag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mOLS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxdshort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullRHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstartlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautolag\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    330\u001b[39m     icbest, bestlag, alres = _autolag(\n\u001b[32m    331\u001b[39m         OLS,\n\u001b[32m    332\u001b[39m         xdshort,\n\u001b[32m   (...)\u001b[39m\u001b[32m    337\u001b[39m         regresults=regresults,\n\u001b[32m    338\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/stonks.ai/.venv/lib/python3.12/site-packages/statsmodels/tsa/stattools.py:133\u001b[39m, in \u001b[36m_autolag\u001b[39m\u001b[34m(mod, endog, exog, startlag, maxlag, method, modargs, fitargs, regresults)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(startlag, startlag + maxlag + \u001b[32m1\u001b[39m):\n\u001b[32m    132\u001b[39m     mod_instance = mod(endog, exog[:, :lag], *modargs)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     results[lag] = \u001b[43mmod_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33maic\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    136\u001b[39m     icbest, bestlag = \u001b[38;5;28mmin\u001b[39m((v.aic, k) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m results.items())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/stonks.ai/.venv/lib/python3.12/site-packages/statsmodels/regression/linear_model.py:333\u001b[39m, in \u001b[36mRegressionModel.fit\u001b[39m\u001b[34m(self, method, cov_type, cov_kwds, use_t, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mpinv\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpinv_wexog\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mnormalized_cov_params\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    331\u001b[39m             \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m         \u001b[38;5;28mself\u001b[39m.pinv_wexog, singular_values = \u001b[43mpinv_extended\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwexog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28mself\u001b[39m.normalized_cov_params = np.dot(\n\u001b[32m    335\u001b[39m             \u001b[38;5;28mself\u001b[39m.pinv_wexog, np.transpose(\u001b[38;5;28mself\u001b[39m.pinv_wexog))\n\u001b[32m    337\u001b[39m         \u001b[38;5;66;03m# Cache these singular values for use later.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/stonks.ai/.venv/lib/python3.12/site-packages/statsmodels/tools/tools.py:264\u001b[39m, in \u001b[36mpinv_extended\u001b[39m\u001b[34m(x, rcond)\u001b[39m\n\u001b[32m    262\u001b[39m x = np.asarray(x)\n\u001b[32m    263\u001b[39m x = x.conjugate()\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m u, s, vt = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43msvd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m s_orig = np.copy(s)\n\u001b[32m    266\u001b[39m m = u.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/stonks.ai/.venv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:1841\u001b[39m, in \u001b[36msvd\u001b[39m\u001b[34m(a, full_matrices, compute_uv, hermitian)\u001b[39m\n\u001b[32m   1837\u001b[39m signature = \u001b[33m'\u001b[39m\u001b[33mD->DdD\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33md->ddd\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1838\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_svd_nonconvergence,\n\u001b[32m   1839\u001b[39m               invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m, over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1840\u001b[39m               under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1841\u001b[39m     u, s, vh = \u001b[43mgufunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1842\u001b[39m u = u.astype(result_t, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1843\u001b[39m s = s.astype(_realType(result_t), copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# --- EXECUTION ---\n",
    "\n",
    "# 1. Load Data\n",
    "df_prices = load_crypto_data(TOKENS, TIMEFRAME)\n",
    "\n",
    "# 2. Run Analysis\n",
    "all_pairs, best_pairs = find_cointegrated_pairs(df_prices)\n",
    "\n",
    "# 3. Display Results\n",
    "print(\"\\nüèÜ --- TOP CANDIDATE PAIRS ---\")\n",
    "if best_pairs.empty:\n",
    "    print(\"No perfect pairs found. Try a different timeframe (e.g., 1h, 15m).\")\n",
    "else:\n",
    "    print(best_pairs)\n",
    "\n",
    "# 4. Visualization of the Best Pair\n",
    "if not best_pairs.empty:\n",
    "    top_pair = best_pairs.iloc[0]\n",
    "    pair_name = top_pair['Pair']\n",
    "    asset_a, asset_b = pair_name.split('-')\n",
    "    ratio = top_pair['Hedge_Ratio']\n",
    "    \n",
    "    # Reconstruct Spread\n",
    "    spread = df_prices[asset_a] - (ratio * df_prices[asset_b])\n",
    "    z_score = (spread - spread.mean()) / spread.std()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot 1: The Z-Score (The Trading Signal)\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(z_score, label=f'Z-Score ({pair_name})', color='purple')\n",
    "    plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.axhline(2, color='red', linestyle='--', alpha=0.5, label='Sell Threshold (+2)')\n",
    "    plt.axhline(-2, color='green', linestyle='--', alpha=0.5, label='Buy Threshold (-2)')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Spread Z-Score: {pair_name}\")\n",
    "    \n",
    "    # Plot 2: The Raw Prices (Visual Check)\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(df_prices[asset_a], label=asset_a, color='orange')\n",
    "    plt.plot(df_prices[asset_b] * ratio, label=f'{asset_b} x {ratio}', color='blue') # Scaled Asset B\n",
    "    plt.legend()\n",
    "    plt.title(f\"Price Alignment (Scaled): {asset_a} vs {asset_b}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
